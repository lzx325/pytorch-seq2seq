{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "from reformer_pytorch import LSHAttention,ReformerLM\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext jupyter_spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n",
      "torch.Size([1, 4, 8320])\n"
     ]
    }
   ],
   "source": [
    "model = ReformerLM(\n",
    "    num_tokens= 20000,\n",
    "    dim = 1024,\n",
    "    depth = 12,\n",
    "    max_seq_len = 8192,\n",
    "    heads = 8,\n",
    "    lsh_dropout = 0.1,\n",
    "    ff_dropout = 0.1,\n",
    "    post_attn_dropout = 0.1,\n",
    "    layer_dropout = 0.1,  # layer dropout from 'Reducing Transformer Depth on Demand' paper\n",
    "    causal = True,        # auto-regressive or not\n",
    "    bucket_size = 64,     # average size of qk per bucket, 64 was recommended in paper\n",
    "    n_hashes = 4,         # 4 is permissible per author, 8 is the best but slower\n",
    "    emb_dim = 128,        # embedding factorization for further memory savings\n",
    "    dim_head = 64,        # be able to fix the dimension of each head, making it independent of the embedding dimension and the number of heads\n",
    "    ff_chunks = 200,      # number of chunks for feedforward layer, make higher if there are memory issues\n",
    "    attn_chunks = 8,      # process lsh attention in chunks, only way for memory to fit when scaling to 16k tokens\n",
    "    num_mem_kv = 128,       # persistent learned memory key values, from all-attention paper\n",
    "    full_attn_thres = 1024, # use full attention if context length is less than set value\n",
    "    reverse_thres = 1024,   # turn off reversibility for 2x speed for sequence lengths shorter or equal to the designated value\n",
    "    use_scale_norm = False,  # use scale norm from 'Transformers without tears' paper\n",
    "    use_rezero = False,      # remove normalization and use rezero from 'ReZero is All You Need'\n",
    "    one_value_head = False,  # use one set of values for all heads from 'One Write-Head Is All You Need'\n",
    "    weight_tie = False,           # tie parameters of each layer for no memory per additional depth\n",
    "    weight_tie_embedding = False, # use token embedding for projection of output, some papers report better results\n",
    "    n_local_attn_heads = 2,       # many papers suggest mixing local attention heads aids specialization and improves on certain tasks\n",
    "    pkm_layers = (4,7),           # specify layers to use product key memory. paper shows 1 or 2 modules near the middle of the transformer is best\n",
    "    pkm_num_keys = 128,           # defaults to 128, but can be increased to 256 or 512 as memory allows\n",
    "    use_full_attn = False    # only turn on this flag to override and turn on full attention for all sequence lengths. for comparison with LSH to show that it is working\n",
    ").cuda()\n",
    "\n",
    "x = torch.randint(0, 20000, (1, 8192)).long().cuda()\n",
    "y = model(x) # (1, 8192, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa\n"
     ]
    }
   ],
   "source": [
    "attn = LSHAttention(\n",
    "    bucket_size = 64,\n",
    "    n_hashes = 16,\n",
    "    causal = True,\n",
    "    return_attn = True\n",
    ")\n",
    "\n",
    "qk = torch.randn(10, 1024, 128)\n",
    "v = torch.randn(10, 1024, 128)\n",
    "\n",
    "out, attn, buckets = attn(qk, v) # (10, 1024, 128)\n",
    "# attn contains the unsorted attention weights, provided return_attn is set to True (costly otherwise)\n",
    "# buckets will contain the bucket number (post-argmax) of each token of each batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch1.6]",
   "language": "python",
   "name": "conda-env-pytorch1.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
